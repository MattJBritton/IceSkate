{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import product\n",
    "import json\n",
    "from math import sqrt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn import datasets\n",
    "\n",
    "#h-stat\n",
    "from sklearn_gbmi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducibility\n",
    "np.random.seed(2019)\n",
    "#functions to parse included datasets\n",
    "def load_bike_dataset():\n",
    "    def _datestr_to_timestamp(s):\n",
    "        return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d\").timetuple())\n",
    "\n",
    "    data = pd.read_csv('data/bike.csv')\n",
    "    data['dteday'] = data['dteday'].apply(_datestr_to_timestamp)\n",
    "    data = pd.get_dummies(data, prefix=[\"weathersit\"], columns=[\"weathersit\"], drop_first=False)\n",
    "\n",
    "    features = ['season',\\\n",
    "                'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',\\\n",
    "                'weathersit_2', 'weathersit_3', 'weathersit_4',\\\n",
    "                'temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "    X = data[features]\n",
    "    y = data['cnt']\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def load_diabetes_dataset():\n",
    "    diabetes_dataset = datasets.load_diabetes()\n",
    "\n",
    "    return pd.DataFrame(diabetes_dataset.data, columns=diabetes_dataset.feature_names), diabetes_dataset.target  \n",
    "\n",
    "def load_boston_dataset():\n",
    "    boston_dataset = datasets.load_boston()\n",
    "\n",
    "    return pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names), boston_dataset.target    \n",
    "\n",
    "def load_dataset(name=\"bike\"):\n",
    "    \n",
    "    if name ==\"bike\":\n",
    "        X,y = load_bike_dataset()\n",
    "    elif name==\"boston\":\n",
    "        X,y = load_boston_dataset() \n",
    "    elif name==\"diabetes\":\n",
    "        X,y = load_diabetes_dataset()           \n",
    "    gbm = GradientBoostingRegressor(min_samples_leaf=10, n_estimators=300)\n",
    "    gbm.fit(X, y) \n",
    "    return X, y, gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25751921089453167"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX, aY, aPred = load_dataset(\"boston\")\n",
    "metrics.r2_score(aY, aPred.predict(aX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from original PyCEBox library\n",
    "#get the x_values for a given granularity of curve\n",
    "def _get_grid_points(x, num_grid_points):\n",
    "    if sorted(list(x.unique())) == [0,1]:\n",
    "        return [0.,1.]\n",
    "    if num_grid_points is None:\n",
    "        return x.unique()\n",
    "    else:\n",
    "        # unique is necessary, because if num_grid_points is too much larger\n",
    "        # than x.shape[0], there will be duplicate quantiles (even with\n",
    "        # interpolation)\n",
    "        return x.quantile(np.linspace(0, 1, num_grid_points)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from original PyCEBox library\n",
    "#average the PDP lines (naive method seems to work fine)\n",
    "def pdp(ice_data):\n",
    "    return ice_data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://nbviewer.jupyter.org/github/alexminnaar/time-series-classification-and-clustering/blob/master/Time%20Series%20Classification%20and%20Clustering.ipynb\n",
    "def _default_factory():\n",
    "    return float(\"inf\")\n",
    "\n",
    "def _get_dtw_distance(s1,s2, w=4):\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    DTW = defaultdict(_default_factory)\n",
    "    DTW[(-1, -1)] = 0\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            DTW[(i, j)] = (s1[i]-s2[j])**2 + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            \n",
    "    return sqrt(DTW[len(s1)-1, len(s2)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform curves before distance measurement\n",
    "def _differentiate(series):\n",
    "        \n",
    "    dS = np.diff(series)\n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for testing random clusters to ensure that algorithm performance is superior\n",
    "def _test_random_clusters(ice_data, num_clusters=5):\n",
    "    temp = np.random.uniform(size=num_clusters)\n",
    "    distribution = temp/temp.sum()\n",
    "    cluster_labels = np.random.choice(a = range(num_clusters),\\\n",
    "                                      size=ice_data.shape[0],\\\n",
    "                                      replace=True, p=distribution)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate lines to num_grid_points when comparing features for feature-space statistics\n",
    "def _interpolate_line(x, y, length):\n",
    "    if len(y) == length:\n",
    "        return y\n",
    "    else:\n",
    "        f = interp1d(x,y, kind=\"cubic\")\n",
    "        return list(f(np.linspace(x[0], x[-1], num=length, endpoint=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_split(columns, model):\n",
    "    split_feature = columns[model.tree_.feature[0]] if model.tree_.value.shape[0] > 1 else 'none'\n",
    "    split_val = round(model.tree_.threshold[0],2)\n",
    "    split_direction = \"<=\" if model.tree_.value.shape[0] == 1\\\n",
    "    or model.classes_[np.argmax(model.tree_.value[1])] == 1 else \">\"\n",
    "    return split_feature, split_val, split_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function - run this to export JSON file for vis\n",
    "def export(data, y, predict_func, num_clusters=5, num_grid_points=40,\\\n",
    "           ice_curves_to_export=100, export_type=\"vis\"):\n",
    "    \n",
    "    #generate data for one ICE plot per column\n",
    "    for column_of_interest in data.columns:\n",
    "        \n",
    "        print column_of_interest\n",
    "\n",
    "        ice_data = pd.DataFrame(np.ones(data.shape[0]), columns=[\"temp\"])\n",
    "    \n",
    "        x_s = _get_grid_points(data[column_of_interest], num_grid_points)      \n",
    "            \n",
    "        #create dataframe with synthetic points (one for each x value returned by _get_grid_points)\n",
    "        for x_val in x_s:\n",
    "            kwargs = {column_of_interest : x_val}\n",
    "            ice_data[x_val] = predict_func(data.assign(**kwargs))\n",
    "        \n",
    "        ice_data.drop(\"temp\", axis=1, inplace=True)\n",
    "        \n",
    "        #center all curves at the min point of the feature's range\n",
    "        ice_data = ice_data.sub(ice_data.mean(axis=1), axis='index')\n",
    "\n",
    "        pdp_data = pdp(ice_data)\n",
    "        if export_type == \"vis\":\n",
    "            hist_counts, hist_bins = np.histogram(a=np.array(data.loc[:,column_of_interest]), bins=\"auto\")\n",
    "            hist_zip = [{\"x\":x[0], \"y\":x[1]} for x in zip(hist_bins, hist_counts)]\n",
    "            export_dict[\"distributions\"][column_of_interest] = hist_zip\n",
    "        export_dict[\"features\"][column_of_interest] = {\"feature_name\": column_of_interest,\n",
    "                                                       \"x_values\": list(x_s),\n",
    "                                                       \"pdp_line\": list(pdp_data),\n",
    "                                                       \"importance\": np.mean(pdp_data),\n",
    "                                                       \"clusters\":[]\n",
    "                                                      }\n",
    "        #perform clustering\n",
    "        ice_data[\"cluster_label\"] = Birch(n_clusters = num_clusters, threshold=0.1)\\\n",
    "                    .fit(_differentiate(ice_data.values)).labels_\n",
    "        ice_data[\"points\"] = ice_data[x_s].values.tolist() \n",
    "\n",
    "        #generate all the ICE curves per cluster\n",
    "        all_curves_by_cluster = ice_data.groupby(\"cluster_label\")[\"points\"].apply(lambda x: np.array(x)) \n",
    "        \n",
    "        splits_first_pass = []\n",
    "        for cluster_num in range(len(all_curves_by_cluster)):                          \n",
    "            num_curves_in_cluster = len(all_curves_by_cluster[cluster_num])\n",
    "\n",
    "            #build model to predict cluster membership\n",
    "            rdwcY = ice_data[\"cluster_label\"].apply(lambda x: 1 if x==cluster_num else 0)\n",
    "            #1-node decision tree to get best split for each cluster\n",
    "            model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1, presort=False,\\\n",
    "                                           class_weight=\"balanced\")\n",
    "            model.fit(data, rdwcY)\n",
    "            split_feature, split_val, split_direction = get_model_split(data.columns, model)\n",
    "            splits_first_pass.append({\"feature\":split_feature, \"val\":split_val,\\\n",
    "                                      \"direction\": split_direction, \"model\": model})\n",
    "       \n",
    "        #loop through splits to find duplicates\n",
    "        duplicate_splits = {}\n",
    "        for i, split_def in enumerate(splits_first_pass[:-1]):\n",
    "            for j, split_def_2 in enumerate(splits_first_pass):\n",
    "                if j<=i or i in duplicate_splits or j in duplicate_splits:\n",
    "                    continue\n",
    "                elif split_def[\"feature\"] == split_def_2[\"feature\"]\\\n",
    "                and split_def[\"direction\"] == split_def_2[\"direction\"]\\\n",
    "                and (split_def[\"val\"] - split_def_2[\"val\"])/(np.ptp(data.loc[:,split_def[\"feature\"]])) <= 0.1:\n",
    "                    duplicate_splits[j] = i\n",
    "        print duplicate_splits\n",
    "        #and split_def[\"val\"] == split_def_2[\"val\"]:\n",
    "        ice_data = ice_data.replace(to_replace={\"cluster_label\":duplicate_splits}, value=None)\n",
    "        #generate all the ICE curves per cluster\n",
    "        all_curves_by_cluster = ice_data.groupby(\"cluster_label\")[\"points\"].apply(lambda x: np.array(x)) \n",
    "        #average the above to get the mean cluster line\n",
    "        cluster_average_curves = {key:np.mean(np.array(list(value)), axis=0)\\\n",
    "                                  for key,value in all_curves_by_cluster.iteritems()}\n",
    "        \n",
    "        for cluster_num in all_curves_by_cluster.keys():                          \n",
    "            num_curves_in_cluster = len(all_curves_by_cluster[cluster_num])\n",
    "\n",
    "            #build model to predict cluster membership\n",
    "            rdwcY = ice_data[\"cluster_label\"].apply(lambda x: 1 if x==cluster_num else 0)\n",
    "            #1-node decision tree to get best split for each cluster\n",
    "            '''\n",
    "            model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1, presort=False,\\\n",
    "                                           class_weight=\"balanced\")\n",
    "            model.fit(data, rdwcY)\n",
    "            '''\n",
    "            model = splits_first_pass[cluster_num][\"model\"]\n",
    "            predY = model.predict(data) \n",
    "            split_feature, split_val, split_direction = get_model_split(data.columns, model)           \n",
    "            \n",
    "            #get random curves if there are more than 100\n",
    "            #no reason to make the visualization display 1000+ curves for large datasets\n",
    "            if num_curves_in_cluster > ice_curves_to_export:\n",
    "                individual_ice_samples = [list(x) for x in\\\n",
    "                                          list(all_curves_by_cluster[cluster_num]\\\n",
    "                                          [np.random.choice(num_curves_in_cluster,\\\n",
    "                                                 size=ice_curves_to_export, replace=False)])\n",
    "                                         ]\n",
    "            else:\n",
    "                individual_ice_samples = [list(x) for x in\\\n",
    "                                          list(all_curves_by_cluster[cluster_num])\\\n",
    "                                         ]\n",
    "            \n",
    "            #add cluster-level metrics to the JSON file\n",
    "            if export_type == \"vis\":\n",
    "                export_dict[\"features\"][column_of_interest][\"clusters\"].append({\n",
    "                    'accuracy': int(round(100.*metrics.accuracy_score(rdwcY, predY))),\n",
    "                    'precision': int(round(100.*metrics.precision_score(rdwcY, predY))),\n",
    "                    'recall': int(round(100.*metrics.recall_score(rdwcY, predY))),\n",
    "                    'split_feature': split_feature,\n",
    "                    'split_val': split_val,\n",
    "                    'split_direction': split_direction,\n",
    "                    'cluster_size': num_curves_in_cluster,\n",
    "                    'line': list(cluster_average_curves[cluster_num]),\n",
    "                    'individual_ice_curves': individual_ice_samples\n",
    "                })\n",
    "            else:\n",
    "                export_dict[\"features\"][column_of_interest][\"clusters\"].append({\n",
    "                    'accuracy': int(round(100.*metrics.accuracy_score(rdwcY, predY))),\n",
    "                    'precision': int(round(100.*metrics.precision_score(rdwcY, predY))),\n",
    "                    'recall': int(round(100.*metrics.recall_score(rdwcY, predY))),\n",
    "                    'split_feature': split_feature,\n",
    "                    'split_val': split_val,\n",
    "                    'split_direction': split_direction,                   \n",
    "                    'predict_function': model.predict, \n",
    "                    'cluster_size': num_curves_in_cluster,\n",
    "                    'line': list(cluster_average_curves[cluster_num])\n",
    "                })                \n",
    "        \n",
    "        #feature-level calculation for cluster distance to pdp\n",
    "        feature_val = export_dict[\"features\"][column_of_interest]\n",
    "        feature_val[\"cluster_deviation\"]\\\n",
    "        = np.mean([_get_dtw_distance(np.array(feature_val[\"pdp_line\"]),\\\n",
    "                                                  np.array(x[\"line\"]))\\\n",
    "                  for x in feature_val[\"clusters\"]])/np.mean(np.array(feature_val[\"pdp_line\"]))\n",
    "        if np.isnan(feature_val[\"cluster_deviation\"]) or feature_val[\"cluster_deviation\"]==float(\"inf\"):\n",
    "            feature_val[\"cluster_deviation\"] = 0\n",
    "        #EOF feature loop\n",
    "        \n",
    "    if export_type == \"vis\":\n",
    "        with open('static/data.json', 'w') as outfile:\n",
    "            json.dump(export_dict, outfile)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc\n",
      "HouseAge\n",
      "AveRooms\n",
      "AveBedrms\n",
      "Population\n",
      "AveOccup\n",
      "Latitude\n",
      "Longitude\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# REGULAR EXPORT\n",
    "dfX, y, predictor = load_dataset(name=\"bike\")\n",
    "export_dict = {\"features\":{}, \"distributions\":{}}\n",
    "export(dfX, y, predictor.predict, num_clusters=3, num_grid_points=20,\\\n",
    "       ice_curves_to_export=2, export_type=\"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pdp(evaluation_set, model_def, columns, offset):\n",
    "    \n",
    "    def generate_interpolation_func(x, y):\n",
    "        return interp1d(x,y, kind=\"linear\")\n",
    "    \n",
    "    funcs = [generate_interpolation_func(model_def[feature][\"x_values\"], model_def[feature][\"pdp_line\"])\\\n",
    "                      for feature in columns]\n",
    "    #from https://stackoverflow.com/questions/52167120/numpy-fastest-way-to-apply-array-of-functions-to-matrix-columns\n",
    "    for i,f in enumerate(funcs):\n",
    "        evaluation_set[:,i] = f(evaluation_set[:,i])\n",
    "    \n",
    "    #print columns\n",
    "    #print evaluation_set.mean(axis=0)\n",
    "    return evaluation_set.sum(axis=1) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6407536081230295\n",
      "0.6916318680534369\n"
     ]
    }
   ],
   "source": [
    "#offset = predictor.predict(np.array(dfX.min(axis=0)).reshape(1,-1))[0]\n",
    "offset = y.mean()\n",
    "eval_samples = np.array(dfX)\n",
    "pdp_model_predictions = predict_pdp(eval_samples, export_dict[\"features\"], dfX.columns, offset)\n",
    "print metrics.r2_score(y, pdp_model_predictions)\n",
    "print metrics.mean_squared_error(y, pdp_model_predictions)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5466117613653548\n",
      "0.7769872876743377\n"
     ]
    }
   ],
   "source": [
    "#offset = predictor.predict(np.array(dfX.min(axis=0)).reshape(1,-1))[0]\n",
    "offset = y.mean()\n",
    "num_samples = dfX.shape[0]\n",
    "eval_sample_ids = np.random.choice(a=dfX.shape[0], size=num_samples, replace=False)\n",
    "eval_samples = np.array(dfX.iloc[eval_sample_ids,:])\n",
    "iceskate_model_predictions = predict_iceskate(eval_samples, export_dict[\"features\"], list(dfX.columns),\\\n",
    "                                              offset)\n",
    "print metrics.r2_score(y[eval_sample_ids], iceskate_model_predictions)\n",
    "print metrics.mean_squared_error(y[eval_sample_ids], iceskate_model_predictions)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iceskate(evaluation_set, model_def, columns, offset):\n",
    "\n",
    "    def generate_interpolation_func(x, y):\n",
    "        if x == [0.,1.]:\n",
    "            def categorical_func(arr, y=y):\n",
    "                return np.where(arr>0,y[1],y[0])\n",
    "            return categorical_func\n",
    "        else:\n",
    "            return interp1d(x,y, kind=\"linear\")    \n",
    "    \n",
    "    def generate_predicate(cluster_def, x_vals, pdp_line, pdp_only=False):\n",
    "        \n",
    "        if pdp_only or cluster_def[\"split_feature\"] == \"none\":\n",
    "            pdp_interpol = generate_interpolation_func(x_vals, pdp_line)\n",
    "            def pdp_func(x, pdp_interpol=pdp_interpol):\n",
    "                return np.full(x.shape[0],pdp_interpol)\n",
    "            return pdp_func\n",
    "        \n",
    "        interpol = generate_interpolation_func(x_vals, cluster_def[\"line\"])\n",
    "        \n",
    "        def output_func(x, predict_func=cluster_def[\"predict_function\"], interpol=interpol):\n",
    "            return np.where(predict_func(x) == 1, interpol, 0) \n",
    "        \n",
    "        return output_func\n",
    "    \n",
    "    funcs = [[generate_predicate(cluster_def, model_def[feature][\"x_values\"],\\\n",
    "                                          model_def[feature][\"pdp_line\"])\\\n",
    "                       for cluster_def in\\\n",
    "                       sorted(model_def[feature][\"clusters\"], reverse=True, key=lambda x: x[\"cluster_size\"])]\\\n",
    "             for feature in columns]\n",
    "    for i_func, possible_split_funcs in enumerate(funcs):\n",
    "        feature = columns[i_func]\n",
    "        possible_split_funcs.append(generate_predicate(None, model_def[feature][\"x_values\"],\\\n",
    "                                          model_def[feature][\"pdp_line\"], True))\n",
    "    \n",
    "    results = np.zeros(shape=(evaluation_set.shape[0], evaluation_set.shape[1]))\n",
    "    \n",
    "    #from https://stackoverflow.com/questions/52167120/numpy-fastest-way-to-apply-array-of-functions-to-matrix-columns\n",
    "    for i_func,possible_split_funcs in enumerate(funcs):\n",
    "        temp = np.zeros(shape=(evaluation_set.shape[0], len(possible_split_funcs)), dtype=object)\n",
    "        for i_inner, func in enumerate(possible_split_funcs):\n",
    "            #temp.append(func(evaluation_set))\n",
    "            temp[:,i_inner] = func(evaluation_set)\n",
    "        \n",
    "        rows_handled = []\n",
    "        func_outputs = []\n",
    "        #from https://stackoverflow.com/questions/11731428/finding-first-non-zero-value-along-axis-of-a-sorted-two-dimensional-numpy-array\n",
    "        for row, valid_func_id in zip(*np.where(temp != 0)):\n",
    "            \n",
    "            valid_func = temp[row, valid_func_id]\n",
    "            \n",
    "            if row in rows_handled:\n",
    "                func_outputs[row].append(valid_func(evaluation_set[row,i_func])[()])\n",
    "            else:\n",
    "                rows_handled.append(row)\n",
    "                func_outputs.append([valid_func(evaluation_set[row,i_func])[()]])\n",
    "                \n",
    "        results[:,i_func] = np.array([(sum(x[:-1] if len(x)>1 else x))/max(len(x[:-1]),1) for x in func_outputs])\n",
    "        #results[:,i_func] = np.array([sum(x[:])/len(x[:]) for x in func_outputs])\n",
    "        #results[:,i_func] = np.array([x[0] for x in func_outputs])\n",
    "    \n",
    "    #print columns\n",
    "    #print results.mean(axis=0)\n",
    "    return results.sum(axis=1) + offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(feature_df).mark_circle().encode( #size=300\n",
    "    x='precision',\n",
    "    y='accuracy',\n",
    "    size='cluster_size',\n",
    "    color=alt.Color('feature_name')\n",
    ").properties(\n",
    "    title=\"Bike Dataset Explanations: Precision vs. Accuracy\",\n",
    "    height=400, \n",
    "    width=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(feature_df).mark_circle(size=100).encode(\n",
    "    x='precision',\n",
    "    y={\n",
    "        \"field\": \"cluster_size\", \n",
    "        \"type\": \"quantitative\",\n",
    "        \"scale\": {\"type\": \"log\"}\n",
    "    },\n",
    "    color=alt.Color('feature_name'),\n",
    ").properties(\n",
    "    title=\"Precision vs. Size\",\n",
    "    height=400, \n",
    "    width=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(feature_df).mark_bar().encode(\n",
    "    x='feature_name',\n",
    "    y='mean(accuracy)'\n",
    ").properties(\n",
    "    title=\"Accuracy by Feature\",\n",
    "    height=400, \n",
    "    width=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(feature_df).mark_circle(size=100).encode(\n",
    "    x='feature_name',\n",
    "    y={\n",
    "        \"field\": \"cluster_size\", \n",
    "        \"type\": \"quantitative\",\n",
    "        \"scale\": {\"type\": \"log\", \"exponent\":0.5}\n",
    "    },\n",
    "    color=alt.Color(field=\"id\", type=\"nominal\", legend=None, scale=alt.Scale(scheme = \"blues-3\")),\n",
    "    order=alt.Order('cluster_size', sort='descending')\n",
    ").properties(\n",
    "    title=\"Cluster Sizes by Feature\",\n",
    "    height=400, \n",
    "    width=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keys_to_cluster(cluster, new_vals):\n",
    "    z = cluster.copy()\n",
    "    z.update(new_vals)\n",
    "    return z\n",
    "\n",
    "feature_list = [l[0] for l in [[add_keys_to_cluster(c, {\"feature_name\":key, \"cluster_num\":i})\\\n",
    "  for i,c in enumerate([v for v in value[\"clusters\"]])]\\\n",
    " for key, value in export_dict[\"features\"].items()] for l[0] in l]\n",
    "\n",
    "feature_df = pd.DataFrame(feature_list).loc[:,\n",
    "                                            [\"feature_name\", \"cluster_num\", \"accuracy\", \"cluster_size\", \n",
    "                                             \"precision\", \"recall\", \"confusion_matrix\",\\\n",
    "                                            \"split_feature\", \"split_val\", \"split_direction\"]]\n",
    "feature_df[\"id\"] = feature_df.apply(lambda x: x[\"feature_name\"]+\": \"\\\n",
    "                                                +x[\"split_feature\"]+x[\"split_direction\"]+str(x[\"split_val\"]),axis=1)\n",
    "\n",
    "#one split\n",
    "print \"mean accuracy: \" + str(np.mean(feature_df[\"accuracy\"]))\n",
    "print \"mean precision: \" + str(np.mean(feature_df[\"precision\"]))\n",
    "print \"mean recall: \" + str(np.mean(feature_df[\"recall\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
