{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "#h-stat\n",
    "from sklearn_gbmi import *\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "#altair\n",
    "import altair as alt\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducibility\n",
    "np.random.seed(2019)\n",
    "#functions to parse included datasets\n",
    "def load_bike_dataset():\n",
    "    def _datestr_to_timestamp(s):\n",
    "        return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d\").timetuple())\n",
    "\n",
    "    data = pd.read_csv('data/bike.csv')\n",
    "    data['dteday'] = data['dteday'].apply(_datestr_to_timestamp)\n",
    "    data = pd.get_dummies(data, prefix=[\"weathersit\"], columns=[\"weathersit\"], drop_first=False)\n",
    "    \n",
    "    #de-normalize data to produce human-readable features.\n",
    "    #Original range info from http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "    data[\"hum\"] = data[\"hum\"].apply(lambda x: x*100.)\n",
    "    data[\"windspeed\"] = data[\"windspeed\"].apply(lambda x: x*67.)\n",
    "    #convert Celsius to Fahrenheit\n",
    "    data[\"temp\"] = data[\"temp\"].apply(lambda x: (x*47. - 8)*9/5 +32)\n",
    "    data[\"atemp\"] = data[\"atemp\"].apply(lambda x: (x*66. - 16)*9/5 + 32)\n",
    "    \n",
    "    #rename features to make them interpretable for novice users\n",
    "    feature_names_dict = {\"yr\":\"First or Second Year\", \n",
    "                              \"season\":\"Season\", \n",
    "                              \"hr\":\"Hour of Day\", \n",
    "                              \"workingday\":\"Work Day\",\n",
    "                              \"weathersit_2\":\"Misty Weather\",\n",
    "                              \"weathersit_3\":\"Light Precipitation\",\n",
    "                              \"weathersit_4\":\"Heavy Precipitation\",\n",
    "                              \"temp\":\"Temperature (F)\",\n",
    "                              \"atemp\":\"Feels Like (F)\",\n",
    "                              \"hum\":\"Humidity\",\n",
    "                              \"windspeed\":\"Wind Speed\"}\n",
    "    data= data.rename(mapper=feature_names_dict,axis=1)\n",
    "    \n",
    "    features = ['yr', 'season', 'hr', 'workingday',\n",
    "                'weathersit_2', 'weathersit_3', 'weathersit_4',\n",
    "                'temp', 'atemp', 'hum', 'windspeed']  \n",
    "    \n",
    "    features = feature_names_dict.values()\n",
    "\n",
    "    X = data[features]\n",
    "    y = data['cnt']\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def load_diabetes_dataset():\n",
    "    diabetes_dataset = datasets.load_diabetes()\n",
    "\n",
    "    return pd.DataFrame(diabetes_dataset.data, columns=diabetes_dataset.feature_names), diabetes_dataset.target  \n",
    "\n",
    "def load_boston_dataset():\n",
    "    boston_dataset = datasets.load_boston()\n",
    "\n",
    "    return pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names), boston_dataset.target    \n",
    "\n",
    "def load_dataset(name=\"bike\"):\n",
    "    \n",
    "    if name ==\"bike\":\n",
    "        X,y = load_bike_dataset()\n",
    "    elif name==\"boston\":\n",
    "        X,y = load_boston_dataset() \n",
    "    elif name==\"diabetes\":\n",
    "        X,y = load_diabetes_dataset()           \n",
    "    gbm = GradientBoostingRegressor(min_samples_leaf=10, n_estimators=300)\n",
    "    gbm.fit(X, y) \n",
    "    return X, y, gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from original PyCEBox library\n",
    "#get the x_values for a given granularity of curve\n",
    "def _get_grid_points(x, num_grid_points):\n",
    "    if sorted(list(x.unique())) == [0,1]:\n",
    "        return [0.,1.], \"categorical\"\n",
    "    if num_grid_points is None:\n",
    "        return x.unique(), \"numeric\"\n",
    "    else:\n",
    "        # unique is necessary, because if num_grid_points is too much larger\n",
    "        # than x.shape[0], there will be duplicate quantiles (even with\n",
    "        # interpolation)\n",
    "        return x.quantile(np.linspace(0, 1, num_grid_points)).unique(), \"numeric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from original PyCEBox library\n",
    "#average the PDP lines (naive method seems to work fine)\n",
    "def _pdp(ice_data):\n",
    "    return ice_data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform curves before distance measurement\n",
    "def _differentiate(series):\n",
    "        \n",
    "    dS = np.diff(series)\n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for testing random clusters to ensure that algorithm performance is superior\n",
    "def _test_random_clusters(ice_data, num_clusters=5):\n",
    "    temp = np.random.uniform(size=num_clusters)\n",
    "    distribution = temp/temp.sum()\n",
    "    cluster_labels = np.random.choice(a = range(num_clusters),\\\n",
    "                                      size=ice_data.shape[0],\\\n",
    "                                      replace=True, p=distribution)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate lines to num_grid_points when comparing features for feature-space statistics\n",
    "def _interpolate_line(x, y, length):\n",
    "    if len(y) == length:\n",
    "        return y\n",
    "    else:\n",
    "        f = interp1d(x,y, kind=\"cubic\")\n",
    "        return list(f(np.linspace(x[0], x[-1], num=length, endpoint=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate VINE curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model_split(columns, model):\n",
    "    split_feature = columns[model.tree_.feature[0]] if model.tree_.value.shape[0] > 1 else 'none'\n",
    "    split_val = round(model.tree_.threshold[0],2)\n",
    "    split_direction = \"<=\" if model.tree_.value.shape[0] == 1\\\n",
    "    or model.classes_[np.argmax(model.tree_.value[1])] == 1 else \">\"\n",
    "    return split_feature, split_val, split_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function - run this to export JSON file for vis\n",
    "def export(data, y, predict_func, num_clusters=5, num_grid_points=40,\\\n",
    "           ice_curves_to_export=100, cluster_method=\"vine\"):\n",
    "    \n",
    "    export_dict = {\"features\":{}, \"distributions\":{}}\n",
    "    \n",
    "    #generate data for one ICE plot per column\n",
    "    for column_of_interest in data.columns:\n",
    "\n",
    "        ice_data = pd.DataFrame(np.ones(data.shape[0]), columns=[\"tempPlaceholderCol\"])\n",
    "    \n",
    "        x_s, column_type = _get_grid_points(data[column_of_interest], num_grid_points)      \n",
    "            \n",
    "        #create dataframe with synthetic points (one for each x value returned by _get_grid_points)\n",
    "        for x_val in x_s:\n",
    "            kwargs = {column_of_interest : x_val}\n",
    "            ice_data[x_val] = predict_func(data.assign(**kwargs))\n",
    "        \n",
    "        ice_data.drop(\"tempPlaceholderCol\", axis=1, inplace=True)\n",
    "        \n",
    "        #center all curves at the mean11 point of the feature's range\n",
    "        ice_data = ice_data.sub(ice_data.mean(axis=1), axis='index')\n",
    "        pdp_data = _pdp(ice_data)\n",
    "        export_dict[\"features\"][column_of_interest] = {\"feature_name\": column_of_interest,\n",
    "                                                       \"x_values\": list(x_s),\n",
    "                                                       \"pdp_line\": list(pdp_data),\n",
    "                                                       \"clusters\":[]\n",
    "                                                      }\n",
    "        \n",
    "        export_dict[\"features\"][column_of_interest][\"ice_data\"] = np.array(ice_data) \n",
    "        \n",
    "        #perform clustering\n",
    "        if cluster_method == \"vine\":\n",
    "            ice_data[\"cluster_label\"] = AgglomerativeClustering(n_clusters = num_clusters)\\\n",
    "                        .fit(_differentiate(ice_data.values)).labels_\n",
    "        elif cluster_method == \"random\":\n",
    "            ice_data[\"cluster_label\"] = _test_random_clusters(ice_data, num_clusters)\n",
    "            \n",
    "        \n",
    "        ice_data[\"points\"] = ice_data[x_s].values.tolist() \n",
    "\n",
    "        #generate all the ICE curves per cluster\n",
    "        all_curves_by_cluster = ice_data.groupby(\"cluster_label\")[\"points\"].apply(lambda x: np.array(x)) \n",
    "        \n",
    "        splits_first_pass = []\n",
    "        for cluster_num in range(len(all_curves_by_cluster)):                          \n",
    "            num_curves_in_cluster = len(all_curves_by_cluster[cluster_num])\n",
    "\n",
    "            #build model to predict cluster membership\n",
    "            rdwcY = ice_data[\"cluster_label\"].apply(lambda x: 1 if x==cluster_num else 0)\n",
    "            #1-node decision tree to get best split for each cluster\n",
    "            model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1, presort=False,\\\n",
    "                                           class_weight=\"balanced\")\n",
    "            model.fit(data, rdwcY)\n",
    "            split_feature, split_val, split_direction = _get_model_split(data.columns, model)\n",
    "            splits_first_pass.append({\"feature\":split_feature, \"val\":split_val,\\\n",
    "                                      \"direction\": split_direction, \"model\": model})\n",
    "       \n",
    "        #loop through splits to find duplicates\n",
    "        duplicate_splits = {}\n",
    "        for i, split_def in enumerate(splits_first_pass[:-1]):\n",
    "            for j, split_def_2 in enumerate(splits_first_pass):\n",
    "                if j<=i or i in duplicate_splits or j in duplicate_splits:\n",
    "                    continue\n",
    "                elif split_def[\"feature\"] == split_def_2[\"feature\"]\\\n",
    "                and split_def[\"direction\"] == split_def_2[\"direction\"]\\\n",
    "                and (split_def[\"val\"] - split_def_2[\"val\"])/(np.ptp(data.loc[:,split_def[\"feature\"]])) <= 0.1:\n",
    "                    duplicate_splits[j] = i\n",
    "\n",
    "        ice_data = ice_data.replace(to_replace={\"cluster_label\":duplicate_splits}, value=None)\n",
    "        #generate all the ICE curves per cluster\n",
    "        all_curves_by_cluster = ice_data.groupby(\"cluster_label\")[\"points\"].apply(lambda x: np.array(x)) \n",
    "        #average the above to get the mean cluster line\n",
    "        cluster_average_curves = {key:np.mean(np.array(list(value)), axis=0)\\\n",
    "                                  for key,value in all_curves_by_cluster.iteritems()}\n",
    "        \n",
    "        for cluster_num in all_curves_by_cluster.keys():                          \n",
    "            num_curves_in_cluster = len(all_curves_by_cluster[cluster_num])\n",
    "\n",
    "            #build model to predict cluster membership\n",
    "            rdwcY = ice_data[\"cluster_label\"].apply(lambda x: 1 if x==cluster_num else 0)\n",
    "            model = splits_first_pass[cluster_num][\"model\"]\n",
    "            predY = model.predict(data) \n",
    "            split_feature, split_val, split_direction = _get_model_split(data.columns, model)   \n",
    "\n",
    "            #get random curves if there are more than 100\n",
    "            #no reason to make the visualization display 1000+ ICE curves for this tool\n",
    "            if num_curves_in_cluster > ice_curves_to_export:\n",
    "                individual_ice_samples = [list(x) for x in\\\n",
    "                                          list(all_curves_by_cluster[cluster_num]\\\n",
    "                                          [np.random.choice(num_curves_in_cluster,\\\n",
    "                                                 size=ice_curves_to_export, replace=False)])\n",
    "                                         ]\n",
    "            else:\n",
    "                individual_ice_samples = [list(x) for x in\\\n",
    "                                          list(all_curves_by_cluster[cluster_num])\\\n",
    "                                         ]\n",
    "            \n",
    "            #add cluster-level metrics to the output dict\n",
    "            export_dict[\"features\"][column_of_interest][\"clusters\"].append({\n",
    "                'accuracy': int(round(100.*metrics.accuracy_score(rdwcY, predY))),\n",
    "                'precision': int(round(100.*metrics.precision_score(rdwcY, predY))),\n",
    "                'recall': int(round(100.*metrics.recall_score(rdwcY, predY))),\n",
    "                'split_feature': split_feature,\n",
    "                'split_val': split_val,\n",
    "                'split_direction': split_direction,                   \n",
    "                'predict_function': model.predict, \n",
    "                'cluster_size': num_curves_in_cluster,\n",
    "                'line': list(cluster_average_curves[cluster_num])\n",
    "            })                \n",
    "        #EOF feature loop\n",
    "     \n",
    "    return export_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_h_statistic(data_frame, model_def, h_stats):\n",
    "    h_stats_dict = {col:{} for col in data_frame.columns}\n",
    "    for key in h_stats.iterkeys():\n",
    "        h_stats_dict[key[0]][key[1]] = round(h_stats[key],3)\n",
    "        h_stats_dict[key[1]][key[0]] = round(h_stats[key],3)\n",
    "        \n",
    "    def get_percent_interactors_in_top_3(feature):\n",
    "        interactors = list([cluster[\"split_feature\"] for cluster in model_def[feature][\"clusters\"]])\n",
    "        if len(interactors) >0 and interactors[0] != \"none\":\n",
    "            top_3 = [x[0] for x in sorted(h_stats_dict[feature].items(), key=lambda kv: kv[1], reverse=True)][:3]\n",
    "            return sum([1 if x in top_3 else 0 for x in interactors]), len(interactors)\n",
    "        else:\n",
    "            return 0, len(interactors)\n",
    "            \n",
    "    sum_of_top_3 = 0.\n",
    "    total_explanations = 0.\n",
    "    baseline_rate = 3./len(data_frame.columns)\n",
    "    for feature in data_frame.columns:\n",
    "        top_3, num_explanations = get_percent_interactors_in_top_3(feature)\n",
    "        sum_of_top_3 += top_3\n",
    "        total_explanations += num_explanations\n",
    "    \n",
    "    return sum_of_top_3/total_explanations, baseline_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ice(data_frame, model_def, offset):\n",
    "    \n",
    "    evaluation_set = np.array(data_frame)\n",
    "    columns = data_frame.columns    \n",
    "    \n",
    "    def generate_interpolation_func(x, y):\n",
    "        return interp1d(x,y, kind=\"linear\")    \n",
    "    \n",
    "    funcs = np.array([[generate_interpolation_func(model_def[feature][\"x_values\"], model_def[feature][\"ice_data\"][row])\\\n",
    "                      for feature in columns] for row in range(dfX.shape[0])]).reshape(dfX.shape[0], dfX.shape[1])\n",
    "    for row in range(funcs.shape[0]):\n",
    "        for col in range(funcs.shape[1]):  \n",
    "            evaluation_set[row,col] = funcs[row, col](evaluation_set[row,col])\n",
    "    \n",
    "    return evaluation_set.sum(axis=1) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pdp(data_frame, model_def, offset):\n",
    "    \n",
    "    evaluation_set = np.array(data_frame)\n",
    "    columns = data_frame.columns    \n",
    "    \n",
    "    def generate_interpolation_func(x, y):\n",
    "        return interp1d(x,y, kind=\"linear\")      \n",
    "    \n",
    "    funcs = [generate_interpolation_func(model_def[feature][\"x_values\"], model_def[feature][\"pdp_line\"])\\\n",
    "                      for feature in columns]\n",
    "    #from https://stackoverflow.com/questions/52167120/numpy-fastest-way-to-apply-array-of-functions-to-matrix-columns\n",
    "    for i,f in enumerate(funcs):\n",
    "        evaluation_set[:,i] = f(evaluation_set[:,i])\n",
    "    \n",
    "    return evaluation_set.sum(axis=1) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vine(data_frame, model_def, offset):\n",
    "    \n",
    "    evaluation_set = np.array(data_frame)\n",
    "    columns = data_frame.columns\n",
    "\n",
    "    def generate_interpolation_func(x, y):\n",
    "        if x == [0.,1.]:\n",
    "            def categorical_func(arr, y=y):\n",
    "                return np.where(arr>0,y[1],y[0])\n",
    "            return categorical_func\n",
    "        else:\n",
    "            return interp1d(x,y, kind=\"linear\")    \n",
    "    \n",
    "    def generate_predicate(cluster_def, x_vals, pdp_line, pdp_only=False):\n",
    "        \n",
    "        if pdp_only or cluster_def[\"split_feature\"] == \"none\":\n",
    "            pdp_interpol = generate_interpolation_func(x_vals, pdp_line)\n",
    "            def pdp_func(x, pdp_interpol=pdp_interpol):\n",
    "                return np.full(x.shape[0],pdp_interpol)\n",
    "            return pdp_func\n",
    "        \n",
    "        interpol = generate_interpolation_func(x_vals, cluster_def[\"line\"])\n",
    "        \n",
    "        def output_func(x, predict_func=cluster_def[\"predict_function\"], interpol=interpol):\n",
    "            return np.where(predict_func(x) == 1, interpol, 0) \n",
    "        \n",
    "        return output_func\n",
    "    \n",
    "    funcs = [[generate_predicate(cluster_def, model_def[feature][\"x_values\"],\\\n",
    "                                          model_def[feature][\"pdp_line\"])\\\n",
    "                       for cluster_def in\\\n",
    "                       sorted(model_def[feature][\"clusters\"], reverse=True, key=lambda x: x[\"cluster_size\"])]\\\n",
    "             for feature in columns]\n",
    "    for i_func, possible_split_funcs in enumerate(funcs):\n",
    "        feature = columns[i_func]\n",
    "        possible_split_funcs.append(generate_predicate(None, model_def[feature][\"x_values\"],\\\n",
    "                                          model_def[feature][\"pdp_line\"], True))\n",
    "    \n",
    "    results = np.zeros(shape=(evaluation_set.shape[0], evaluation_set.shape[1]))\n",
    "    \n",
    "    #from https://stackoverflow.com/questions/52167120/numpy-fastest-way-to-apply-array-of-functions-to-matrix-columns\n",
    "    for i_func,possible_split_funcs in enumerate(funcs):\n",
    "        temp = np.zeros(shape=(evaluation_set.shape[0], len(possible_split_funcs)), dtype=object)\n",
    "        for i_inner, func in enumerate(possible_split_funcs):\n",
    "            temp[:,i_inner] = func(evaluation_set)\n",
    "        \n",
    "        rows_handled = []\n",
    "        func_outputs = []\n",
    "        #from https://stackoverflow.com/questions/11731428/finding-first-non-zero-value-along-axis-of-a-sorted-two-dimensional-numpy-array\n",
    "        for row, valid_func_id in zip(*np.where(temp != 0)):\n",
    "            \n",
    "            valid_func = temp[row, valid_func_id]\n",
    "            \n",
    "            if row in rows_handled:\n",
    "                func_outputs[row].append(valid_func(evaluation_set[row,i_func])[()])\n",
    "            else:\n",
    "                rows_handled.append(row)\n",
    "                func_outputs.append([valid_func(evaluation_set[row,i_func])[()]])\n",
    "                \n",
    "        results[:,i_func] = np.array([(sum(x[:-1] if len(x)>1 else x))/max(len(x[:-1]),1) for x in func_outputs])\n",
    "\n",
    "    return results.sum(axis=1) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_scores(y, predictions, offset):\n",
    "    return {\"R-Squared\": metrics.r2_score(y, predictions),\n",
    "            \"MSE\":  (metrics.mean_squared_error(y, predictions)**.5)/offset\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keys_to_cluster(cluster, new_vals):\n",
    "    z = cluster.copy()\n",
    "    z.update(new_vals)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run VINE for 3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE RESULT DATA FOR CHARTS\n",
    "results = {}\n",
    "for dataset in [\"diabetes\", \"boston\", \"bike\"]:\n",
    "    print dataset\n",
    "    results[dataset] = {}\n",
    "    dfX, y, predictor = load_dataset(name=dataset)\n",
    "    VINE_data = export(dfX, y, predictor.predict,\n",
    "                       num_clusters=5,\n",
    "                       num_grid_points=20,\n",
    "                       cluster_method=\"vine\",\n",
    "                       log=True\n",
    "                      )\n",
    "    VINE_data_random = export(dfX, y, predictor.predict,\n",
    "                       num_clusters=5,\n",
    "                       num_grid_points=20,\n",
    "                       cluster_method=\"random\"\n",
    "                      )\n",
    "    #basic dataset statistics\n",
    "    results[dataset][\"model_r2_score\"] = metrics.r2_score(y, predictor.predict(dfX))   \n",
    "    results[dataset][\"dataset_instances\"] = dfX.shape[0]\n",
    "    results[dataset][\"dataset_features\"] = dfX.shape[1]\n",
    "    #H-statistic score\n",
    "    raw_h_stats = h_all_pairs(predictor, dfX)\n",
    "    h_stat_score, h_stat_baseline = compare_to_h_statistic(dfX, VINE_data[\"features\"], raw_h_stats)\n",
    "    results[dataset][\"h_stat_score\"] = h_stat_score\n",
    "    results[dataset][\"h_stat_baseline\"] = h_stat_baseline\n",
    "    \n",
    "    #Explanation accuracy metrics\n",
    "    feature_df = pd.DataFrame([l[0] for l in [[add_keys_to_cluster(c, {\"feature_name\":key, \"cluster_num\":i})\\\n",
    "      for i,c in enumerate([v for v in value[\"clusters\"]])]\\\n",
    "     for key, value in VINE_data[\"features\"].items()] for l[0] in l])\n",
    "    feature_df_random = pd.DataFrame([l[0] for l in [[add_keys_to_cluster(c, {\"feature_name\":key, \"cluster_num\":i})\\\n",
    "      for i,c in enumerate([v for v in value[\"clusters\"]])]\\\n",
    "     for key, value in VINE_data_random[\"features\"].items()] for l[0] in l])    \n",
    "    \n",
    "    results[dataset][\"scores\"] = {}\n",
    "    for metric in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "        results[dataset][\"scores\"][metric] = {}\n",
    "        results[dataset][\"scores\"][metric][\"explanation\"] = np.mean(feature_df.loc[:,metric])\n",
    "        results[dataset][\"scores\"][metric][\"random\"] = np.mean(feature_df_random.loc[:,metric])\n",
    "        \n",
    "    #ICE,PDP, and VINE Information Ceiling scores\n",
    "    offset = y.mean()\n",
    "    #ICE\n",
    "    results[dataset][\"ice_score\"] = get_prediction_scores(y, \n",
    "                                                          predict_ice(dfX, VINE_data[\"features\"], offset),\n",
    "                                                          offset)\n",
    "    #PDP\n",
    "    results[dataset][\"pdp_score\"] = get_prediction_scores(y, \n",
    "                                                          predict_pdp(dfX, VINE_data[\"features\"], offset),\n",
    "                                                          offset)  \n",
    "    #Run VINE for each cluster size\n",
    "    results[dataset][\"vine_score\"] = {}\n",
    "    for c in range(3,8):\n",
    "        VINE_data_loop = export(dfX, y, predictor.predict,\n",
    "                           num_clusters=c,\n",
    "                           num_grid_points=20,\n",
    "                           cluster_method=\"vine\"\n",
    "                          ) \n",
    "        results[dataset][\"vine_score\"][c] = get_prediction_scores(y, \n",
    "                                                          predict_vine(dfX, VINE_data_loop[\"features\"], offset),\n",
    "                                                          offset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect raw results JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results For Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_accuracy = []\n",
    "for dataset, result in results.iteritems():\n",
    "    for score_type, score in result[\"scores\"].iteritems():\n",
    "        explanation_accuracy.append({\n",
    "            \"Dataset\":dataset,\n",
    "            \"Metric\":score_type,\n",
    "            \"Condition\": \"Cluster Explanation\",\n",
    "            \"Score\": score[\"explanation\"]\n",
    "        })\n",
    "        explanation_accuracy.append({\n",
    "            \"Dataset\":dataset,\n",
    "            \"Metric\":score_type,\n",
    "            \"Condition\": \"Random Baseline\",\n",
    "            \"Score\": score[\"random\"]\n",
    "        })        \n",
    "        \n",
    "explanation_accuracy_df = pd.DataFrame(explanation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_ceiling = []\n",
    "for dataset, result in results.iteritems():\n",
    "    for i in range(3,8):\n",
    "        information_ceiling.append({\n",
    "            \"Dataset\":dataset,\n",
    "            \"Number of Clusters\": i,\n",
    "            \"Visualization\": \"PDP\",\n",
    "            \"R2\": result[\"pdp_score\"][\"R-Squared\"]\n",
    "        })\n",
    "        information_ceiling.append({\n",
    "            \"Dataset\":dataset,\n",
    "            \"Number of Clusters\": i,\n",
    "            \"Visualization\": \"ICE\",\n",
    "            \"R2\": result[\"ice_score\"][\"R-Squared\"]\n",
    "        })\n",
    "    for key, val in result[\"vine_score\"].iteritems():\n",
    "        information_ceiling.append({\n",
    "            \"Dataset\":dataset,\n",
    "            \"Number of Clusters\": key,\n",
    "            \"Visualization\": \"VINE\",\n",
    "            \"R2\": val[\"R-Squared\"]\n",
    "        })    \n",
    "information_ceiling_df = pd.DataFrame(information_ceiling)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charts and Figures for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(explanation_accuracy_df).mark_bar(opacity=0.7).encode(\n",
    "    x='Metric:O',\n",
    "    y=alt.Y('Score:Q', stack=None),\n",
    "    column='Dataset:N',\n",
    "    color=\"Condition:N\"\n",
    ").properties(\n",
    "    title=\"Accuracy, Precision, and Recall of Cluster Explanations vs Random Baseline\",\n",
    "    height=200, \n",
    "    width=150\n",
    ").configure_axisBottom(labelAngle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(information_ceiling_df).mark_line().encode(\n",
    "    x=\"Number of Clusters\",\n",
    "    y=\"R2\",\n",
    "    color=\"Visualization\"\n",
    ").properties(\n",
    "    height=300, \n",
    "    width=200\n",
    ").facet(column=\"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 2D PDP plot on bike dataset\n",
    "sample_dfX, sample_y, sample_predictor = load_dataset(name=\"bike\")\n",
    "fig, axs = plot_partial_dependence(sample_predictor, sample_dfX, [(0,7)],\n",
    "                                   feature_names=sample_dfX.columns,\n",
    "                                   grid_resolution=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample 1D PDP Plots on bike dataset\n",
    "fig, axs = plot_partial_dependence(sample_predictor, sample_dfX, [0,1,7],\n",
    "                                   feature_names=sample_dfX.columns,\n",
    "                                   grid_resolution=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
